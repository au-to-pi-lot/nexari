# Bot Configuration
bot:
  discord:
    bot_token: your_discord_bot_token_here  # Your Discord bot token for authentication, get here https://discord.com/developers/applications
    client_id: your_discord_client_id_here  # Your Discord application's client ID, ditto
  chat:
    context_length: 1000  # The number of tokens the LLM is capable of holding in context
    message_limit: 20  # The maximum number of messages to fetch for context
  llms:
    name: gpt3
    api_base: https://api.openai.com/v1  # The base URL for the LLM API
    llm_name: gpt-3.5-turbo  # The name of the LLM model to use
    api_key: your_api_key_here  # Your API key for authentication with the LLM provider
    max_tokens: 100  # The maximum number of tokens to generate in each response (note, there is some overhead!)
    sampling:
      # Not all APIs support all sampling parameters. Comment out any which result in errors.
      temperature: 0.7  # Controls randomness in the output (0.0 = deterministic, 1.0+ = very random)
      top_p: 1.0  # Nucleus sampling: only consider the top p% of probability mass (default: 1.0, i.e., off)
      top_k: 0  # Only consider the top k most likely tokens (default: 0, use 0 to turn off)
      frequency_penalty: 0.0  # Penalize new tokens based on their frequency in the text so far (-2.0 to 2.0)
      presence_penalty: 0.0  # Penalize new tokens based on whether they appear in the text so far (-2.0 to 2.0)
      repetition_penalty: 1.0  # Penalize repetitions (1.0 means no penalty, higher values = stronger penalty)
      min_p: 0.0  # Minimum probability for a token to be considered (0.0 to 1.0, 0.0 means off)
      top_a: 0.0  # Consider tokens with probability > (max_prob * top_a) (0.0 to 1.0, 0.0 means off)

# You can add more LLM configurations as needed by following the structure above
