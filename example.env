# Discord Bot Token (Required)
# This is the token for your Discord bot, which you can obtain from the Discord Developer Portal
DISCORD_BOT_TOKEN=your_discord_bot_token_here

# Discord Bot Client ID (Required)
# This is the client ID for your Discord bot, which you can obtain from the Discord Developer Portal
DISCORD_CLIENT_ID=your_discord_client_id_here

# Llama Model Path (Optional, default: models/model.bin)
# The path to your llama.cpp compatible model file
LLAMA_MODEL_PATH=models/model.bin

# Llama Model URL (Optional)
# The URL to download the model if it's not available locally
LLAMA_MODEL_URL=https://example.com/path/to/your/model.bin

# Maximum Tokens (Optional, default: 100)
# The maximum number of tokens to generate in the bot's response
MAX_TOKENS=100

# Chat Template (Optional, default: llama-3)
# The chat template to use for formatting messages.
# Options: llama-2, llama-3, alpaca, qwen, vicuna, oasst_llama, baichuan-2, baichuan, openbuddy,
#   redpajama-incite, snoozy, phind, intel, open-orca, mistrallite, zephyr, pygmalion, chatml, mistral-instruct,
#   chatglm3, openchat, saiga, gemma
CHAT_TEMPLATE=llama-3

# Temperature (Optional, default: 0.7)
# Controls randomness in generation. Higher values make output more random, lower values make it more deterministic
TEMPERATURE=0.7

# Context Length (Optional, default: 1000)
# The maximum number of tokens to include in the conversation history
CONTEXT_LENGTH=1000

# Stop Tokens (Optional, default: None)
# A comma-separated list of tokens that will stop the generation when encountered
STOP_TOKENS=

# GPU Layers (Optional, default: 0)
# The number of model layers to store on the GPU. Set to 0 to use CPU only.
GPU_LAYERS=0

# Enable Flash Attention (Optional, default: false)
# Set to true to enable Flash Attention for faster processing on supported GPUs
ENABLE_FLASH_ATTENTION=false

# Enable Tensorcores (Optional, default: false)
# Set to true to enable Tensorcores support for faster processing on supported NVIDIA GPUs
ENABLE_TENSORCORES=false

